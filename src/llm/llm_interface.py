# src/llm/llm_interface.py
"""
Interface for interacting with the Large Language Model (LLM).

Handles loading the model (e.g., Mistral 7B GGUF), configuring it 
(GPU layers, context size, etc.), and performing inference.

Inputs:
- LLM configuration dictionary (from config.yaml)
- User prompts (strings)

Outputs:
- Generated text responses (strings)
"""

import logging
from llama_cpp import Llama # Assuming use of llama-cpp-python

class LLMInterface:
    def __init__(self, llm_config):
        self.config = llm_config
        self.model = None
        self._load_model()

    def _load_model(self):
        """Loads the GGUF model based on the configuration."""
        logging.info(f"Loading LLM from: {self.config['model_path']}")
        try:
            # Actual model loading logic using llama-cpp-python
            self.model = Llama(
                model_path=self.config['model_path'],
                n_gpu_layers=self.config['n_gpu_layers'],
                n_ctx=self.config['n_ctx'],
                # f16_kv=self.config.get('fp16', True), # Use fp16 key-value cache if enabled in config
                verbose=False # Set verbosity as needed
            )
            logging.info("LLM loaded successfully.")
        except Exception as e:
            logging.error(f"Failed to load LLM: {e}", exc_info=True)
            # Depending on the application design, you might want to exit
            # or handle this more gracefully.
            raise

    def generate(self, prompt):
        """Generates a response from the LLM based on the prompt, yielding tokens.

        Yields:
            str: Each token generated by the model.
        """
        if not self.model:
            logging.error("LLM not loaded. Cannot generate text.")
            yield "Error: LLM not available."
            return

        logging.debug(f"Generating streaming response for prompt: {prompt[:50]}...")
        try:
            # Use stream=True to get a generator
            stream = self.model(
                prompt,
                max_tokens=self.config['max_tokens'],
                temperature=self.config['temperature'],
                stop=["User:", "Assistant:"], # Adjust stop sequences as needed
                stream=True
            )

            # Yield each token's text as it arrives
            for output in stream:
                token_text = output['choices'][0]['text']
                # logging.debug(f"Streamed token: {repr(token_text)}") # Log each token if needed
                yield token_text

        except Exception as e:
            logging.error(f"Error during LLM streaming generation: {e}", exc_info=True)
            yield "Error: Could not generate response."

# Example usage (for testing):
if __name__ == '__main__':
    # This part would require a dummy config for direct execution
    logging.basicConfig(level=logging.INFO)
    dummy_config = {
        'model_path': 'dummy/path/model.gguf',
        'n_gpu_layers': 0, 
        'n_ctx': 512,
        'max_tokens': 100,
        'temperature': 0.7
    }
    
    # In a real scenario, loading might fail if the path is invalid or llama-cpp-python is not installed
    try:
        # Note: This example still uses a dummy config and simulates the model object
        # for direct execution without needing the actual model file here.
        # To test the real loading, run the main application.
        class MockLLMInterface:
            def __init__(self, config):
                self.config = config
                print(f"Simulating LLM load with config: {config}")
                self.model = "Simulated Llama Model Object"
            def generate(self, prompt):
                print(f"Simulating streaming generation for: {prompt[:50]}...")
                # Simulate streaming by yielding parts of the response
                response = f"Simulated response to: {prompt}"
                import time
                for char in response:
                    yield char
                    time.sleep(0.02) # Simulate delay

        llm_interface = MockLLMInterface(dummy_config)
        prompt = "Explain the concept of Large Language Models in simple terms."
        print(f"Prompt: {prompt}")
        print("Response (streaming): ", end="")
        full_response = ""
        for token in llm_interface.generate(prompt):
            print(token, end="", flush=True)
            full_response += token
        print() # Newline after streaming is done
        print(f"\nFull collected response: {full_response}")
    except Exception as e:
        print(f"Could not run example: {e}")